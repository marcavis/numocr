    A primeira consideração tomada na implementação do algoritmo foi decidir a
quantidade de neurônios na camada intermediária - a média geométrica pareceu ser
interessante para isso, mas, conforme o desenvolvimento prosseguiu, viu-se
que um número menor era suficiente.
    Uma preocupação inicial foi sempre evitar enganos nos índices; felizmente,
a sintaxe de Python é razoavelmente compacta, portanto é possível escrever
código que se pareça com as funções matemáticas descritas no algoritmo da
apostila, sem ser obscurecido por loops - até certo ponto, pois o preenchimento
de matrizes fica mais claro usando loops para uma das dimensões.
    Um problema encontrado, no momento em que o algoritmo chegou no cálculo de
z_in, foi que o valor que chegava para a função sigmoide era sempre muito alto,
na faixa de 30, o que sempre levava a um resultado muito próximo a 1. Tentou-se,
sem muita convicção, dividir os valores de z_in por algum fator, a título de
normalização, para que o resultado da função sigmoide se aproximasse de 0.5,
sendo portanto valores mais adequados para se trabalhar.
    Só no dia seguinte é notado que a função de geração de pesos aleatórios
estava gerando pesos apenas entre 0.0 e 1.0, o que explicava essa tendência
positiva exagerada.
    Os valores continuam muito altos, então tentaremos uma normalização das
entradas, para que valores de 0 a 255 fiquem compreendidos entre 0.0 e 1.0.
    Em algum ponto, uma preocupação com a velocidade do código aparece, então
testamos a função time.time() da biblioteca Python para verificar qual a parte
mais pesada do código.
    Processamento de uma linha: 28 ms, dos quais 8.8 ms são para atualizar os
pesos da primeira camada, e 0.2 ms para atualizar os pesos da segunda camada.
Talvez haja algo melhor que um loop?
    De fato, atualizar pesos linha por linha, usando list comprehensions, como:
    pesosV[i] = [x + y for (x,y) in zip(pesosV[i], delta_v[i])]
    Economiza 3 ms por linha processada.
    Porém, uma otimização bem mais potente seria diminuir a quantidade de
neurônios intermediários - uma diminuição em 50% diminui o tempo de
processamento por linha em cerca de 45%, e tendo uma piora apenas leve na
diminuição de erro médio por linha.
    Para testar o acerto do algoritmo, colocamos um limiar de 0.7 (seria
arrojado demais? Conservador demais? Há de se testar). Assim, se o resultado de
um dos itens da camada final for maior que o limiar, ele será convertido em uma
resposta positiva; para testar se a resposta foi correta, convertemos o vetor
da resposta em um vetor de booleanos (depois transformados em 0 e 1), e
verificamos se é completamente igual à resposta esperada; os erros possíveis
são: ausência de itens 1 na resposta, item 1 incorreto na resposta, e presença
de mais de um item 1 na resposta.
    Com o uso de algumas variáveis de depuração, encontramos os seguintes níveis
de acerto num teste usando a base com 10 mil linhas, rodando por 10 épocas, com
limiar de 0.7:
    1ª época: 52% de acerto
    2ª época: 77% de acerto
    3ª época: 82% de acerto
    4ª época: 86% de acerto
    5ª época: 87% de acerto
    6ª época: 88% de acerto
    7ª época: 89% de acerto
    8ª época: 90% de acerto
    9ª época: 91% de acerto
    10ª época: 92% de acerto
